{"backend_state":"ready","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-a3900a97-1597-472c-8ced-b21ab6c160d6.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1722963975488,"trust":true,"type":"settings"}
{"cell_type":"code","end":1715952548705,"exec_count":75,"id":"3e5651","input":"param_grid = {\n 'min_samples_leaf': np.arange(1,4),\n 'max_depth': np.arange(2, 10),\n 'n_estimators': np.arange(10,100, 10),\n }\n\ngrid = gridsearch(classify(), param_grid)\n\ngrid.fit(Xtrain, ytrain)\n\ngrid.best_params_","kernel":"python3","output":{"0":{"data":{"text/plain":"{'max_depth': 9, 'min_samples_leaf': 3, 'n_estimators': 30}"},"exec_count":75}},"pos":3.90625,"scrolled":false,"start":1715952347637,"state":"done","type":"cell"}
{"cell_type":"code","end":1715952566161,"exec_count":76,"id":"886398","input":"randforest = classify(criterion='gini', max_depth=9, min_samples_leaf=3, n_estimators=30).fit(Xtrain,ytrain)\nforestpred = randforest.predict(Xtest)","kernel":"python3","pos":3.9150390625,"start":1715952565965,"state":"done","type":"cell"}
{"cell_type":"code","end":1715952569634,"exec_count":77,"id":"458812","input":"\nmat = confusion_matrix(ytest, forestpred)\n\nfig = plt.figure(figsize=(9, 9))\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues', xticklabels = data['specClass'].unique(), yticklabels = data['specClass'].unique())\nplt.xlabel('true label')\nplt.ylabel('predicted label');","kernel":"python3","output":{"0":{"data":{"image/png":"4bdffae273d30a130fad5d8bdf5e336a81be8be8","text/plain":"<Figure size 648x648 with 1 Axes>"},"metadata":{"image/png":{"height":533,"width":533},"needs_background":"light"}}},"pos":3.9296875,"scrolled":false,"start":1715952569114,"state":"done","type":"cell"}
{"cell_type":"code","end":1715957282566,"exec_count":3,"id":"e8ba92","input":"import pandas as pd\nimport neural as nl\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.ensemble import RandomForestClassifier as classify\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV as gridsearch\n\ndata = pd.read_csv('./data/SDSS_galaxies.csv')\ndata.head()","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>u</th>\n      <th>g</th>\n      <th>r</th>\n      <th>i</th>\n      <th>z</th>\n      <th>specClass</th>\n      <th>redshift</th>\n      <th>redshift_err</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.31537</td>\n      <td>17.03378</td>\n      <td>16.43044</td>\n      <td>16.07336</td>\n      <td>15.79528</td>\n      <td>GALAXY</td>\n      <td>0.057006</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.56470</td>\n      <td>17.89683</td>\n      <td>16.79172</td>\n      <td>16.30632</td>\n      <td>15.94328</td>\n      <td>GALAXY</td>\n      <td>0.152668</td>\n      <td>0.000030</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.25199</td>\n      <td>19.00241</td>\n      <td>18.72587</td>\n      <td>18.55749</td>\n      <td>18.58451</td>\n      <td>QSO</td>\n      <td>1.448379</td>\n      <td>0.000437</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19.30206</td>\n      <td>17.12601</td>\n      <td>16.01106</td>\n      <td>15.51805</td>\n      <td>15.05948</td>\n      <td>GALAXY</td>\n      <td>0.105601</td>\n      <td>0.000025</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18.39921</td>\n      <td>17.21244</td>\n      <td>16.64964</td>\n      <td>16.36866</td>\n      <td>16.19744</td>\n      <td>GALAXY</td>\n      <td>0.032242</td>\n      <td>0.000042</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          u         g         r         i         z specClass  redshift  \\\n0  18.31537  17.03378  16.43044  16.07336  15.79528    GALAXY  0.057006   \n1  19.56470  17.89683  16.79172  16.30632  15.94328    GALAXY  0.152668   \n2  19.25199  19.00241  18.72587  18.55749  18.58451       QSO  1.448379   \n3  19.30206  17.12601  16.01106  15.51805  15.05948    GALAXY  0.105601   \n4  18.39921  17.21244  16.64964  16.36866  16.19744    GALAXY  0.032242   \n\n   redshift_err  \n0      0.000007  \n1      0.000030  \n2      0.000437  \n3      0.000025  \n4      0.000042  "},"exec_count":3}},"pos":2.5,"scrolled":false,"start":1715957279850,"state":"done","type":"cell"}
{"cell_type":"code","end":1715957283247,"exec_count":4,"id":"993d9e","input":"QSOs = data.query('specClass == \\'QSO\\'')\nGalaxies = data.query('specClass == \\'GALAXY\\'')\n\n\nplt.plot(QSOs['redshift'][:-150:150], 'o', label='QSOs')\nplt.plot(Galaxies['redshift'][:-150:150], 'o', label='Galaxies')\nplt.grid()\nplt.ylabel('Redshift')\nplt.legend()\nplt.show()\npass","kernel":"python3","output":{"0":{"data":{"image/png":"f957398bc42d4b4760ddaf1a6a29c0d844e1b42e","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":710},"needs_background":"light"}}},"pos":2.6875,"start":1715957282591,"state":"done","type":"cell"}
{"cell_type":"code","end":1715957285657,"exec_count":5,"id":"f3e64b","input":"def mod(data):\n    '''\n    Input\n    ------\n    data: pandas dataframe\n        my pandas array that I want to find the differences of\n\n    Output\n    -------\n    new data: numpy array\n        The new data frame with all of the differences. Also cuts out any columns I'm not interested in applying an operation to.\n    '''\n    labels = list('ugriz')\n    new_labels = [f'{labels[i]} - {labels[i+1]}' for i in range(len(labels)-1)]\n    new_data = np.array([data[f'{labels[i]}']-data[f'{labels[i+1]}'] for i in range(len(labels)-1)]).squeeze()\n    return new_data.T\n\npass","kernel":"python3","pos":2.875,"scrolled":false,"start":1715957285646,"state":"done","type":"cell"}
{"cell_type":"code","end":1715957288027,"exec_count":6,"id":"290313","input":"Features= np.array([(*x,y) for x,y in zip(mod(data),data['redshift'])]).squeeze()\nScaledFeatures = StandardScaler().fit_transform(Features)\nSampledFeatures, SampledClasses = RandomUnderSampler().fit_resample(ScaledFeatures, data['specClass'])\n\n\nXtrain, Xtest, ytrain, ytest = train_test_split(SampledFeatures, SampledClasses, train_size=300)","kernel":"python3","pos":3.25,"scrolled":false,"start":1715957287714,"state":"done","type":"cell"}
{"cell_type":"code","end":1715957435438,"exec_count":12,"id":"139e44","input":"mat = confusion_matrix(neuralnetwork.truths, neuralnetwork.predictions)\n\nfig = plt.figure(figsize=(9, 9))\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues', xticklabels=neuralnetwork.labels, yticklabels=neuralnetwork.labels)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","kernel":"python3","output":{"0":{"data":{"image/png":"de9abc532a8586b2512d7549bc1ee401e261d940","text/plain":"<Figure size 648x648 with 1 Axes>"},"metadata":{"image/png":{"height":533,"width":533},"needs_background":"light"}}},"pos":5,"scrolled":false,"start":1715957435032,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0a2aad","input":"_Seems as though redshifts can be a great input into telling whether an object is a QSO or galaxy. I've decided to use the redshift data as well as the colours._","pos":2.78125,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"2c96a4","input":"**_Optimisation_**\n- _As this is a binary classification problem, the random forest will not be very complex._\n- _This means that only a few hyperparameters would actually effect the accuracy of the model._\n- _From previous tests it is known that the gini criterion is best_\n- _Just need to explore max depth and number of estimators_","pos":3.859375,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"2cca20","input":"**Step 3**: Build my testing and training data set.\n _This involved 3 steps_: \n - _Step 1: build my feature set with my colours and redshift_\n - _Step 2: scale all of my data since it's always a good idea to do so_\n - _Step 3: I've been given an unbalanced data set so I've used imblearn to under sample and achieve a balanced data set_","pos":3.0625,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"2f0ed0","input":"**Optimisation**\n\n_Due to the neural network being a home-brewed solution, I see no other way of optimising the network other than brute force trying different combinations of network parameters manually_","pos":3.96484375,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"37b314","input":"**Step 2**: I have the magnitude data but I need the differences in magnitude.\n- Simply build a function that will find the differences of my columns","pos":2.828125,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"50a334","input":"_Assessing_","pos":4.5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"680121","input":"While the neural network can achieve incredibly high accuracies (~98%) it does not seem to be the right choice for this task. The network takes longer to train than the random forest classifier, leading to longer optimisation sweeps. Additionally, the neural network has a bad habit of overfitting data and has no method by which to self-correct. This means that sometimes when training the network it will assign all inputs as a single class. Meanwhile the random forest classifier can achieve as good if not better accuracy (~97%) while avoiding these issues.\n\nDespite this, there are key differences in the way the two models make mistakes. The random forest seems to err on the side of caution, having more false negatives than positives (1:3 ratio for false positives to negatives), on the other hand the neural network has the opposite effect outputting more false positives (3:1 ratio for false positives to negatives).","pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"81477b","input":"# Neural Network","pos":3.953125,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"9471e1","input":"# **Discussion**","pos":6,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a0b082","input":"**Testing**\n","pos":3.912109375,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"ac94d0","input":"# **Project**\n\n**step 1**: import all dependencies and inspect data\n- _Seems as though there's extra data here so will plot to see what it means_","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b14351","input":"_Assessing_","pos":3.91796875,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d58c1a","input":"_Training_","pos":3.9765625,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e25777","input":"# Random Forest","pos":3.8125,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f4c2ab","input":"**Step 4**: Training and optimising\n\n_Now to train both my neural network and a random forest classifier then apply some optimisation techniques. I've decided to use a brute force method of using a grid search to ensure maximum prediction accuracy._\n\n**Step 5**: Assessing.\n\n_I use a confusion matrix to assess the performance of my neural network_","pos":3.625,"state":"done","type":"cell"}
{"end":1715957431710,"exec_count":11,"id":"46d63f","input":"neuralnetwork = nl.Network([4])\nneuralnetwork.SGD(Xtrain, ytrain, Xtest, ytest, epochs=60, eta=10**-2)","kernel":"python3","output":{"0":{"data":{"text/plain":"'Training score of:  0.96, Test score of:  0.96'"},"exec_count":11}},"pos":4,"scrolled":false,"start":1715957405907,"state":"done","type":"cell"}
{"id":0,"time":1722963975278,"type":"user"}
{"last_load":1715176384981,"type":"file"}