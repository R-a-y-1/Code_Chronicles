{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Q1**\n",
    "\n",
    "**Step 1**: Load in the the csv into a `dataframe`.  \n",
    "\n",
    "**Step 2**: Assess how many unique values of class there are.  \n",
    "\n",
    "_Note: cannot query data using_ _`data.query()`_ _as 'class' is a python keyword_  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./data/pulsar.csv', index_col=0)\n",
    "number_RFI = len(data[data['class'] == 0])\n",
    "number_pulsars = len(data[data['class'] == 1])\n",
    "\n",
    "print(f'The number of pulsars is {number_pulsars}, and the number of noise data points is {number_RFI}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 3**: Check the range of standard deviations and means in the data set.\n",
    "\n",
    "_It seems that the standard deviations are not of order 1, nor are the means of order 0, as such we will have to standardise our data before analysing to make sure the model is not over emphasising the high variance/high mean data._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "dev0 = data['std_dm']\n",
    "dev1 = data['std_pf']\n",
    "mean0 = data['mean_dm']\n",
    "mean1 = data['mean_pf']\n",
    "\n",
    "print(f'Dispersion Dev  : {max(dev0)}\\t{min(dev0)} \\nPulse Dev\\t: {max(dev1)}\\t{min(dev1)}')\n",
    "print(f'\\n\\nDispersion Mean : {max(mean0)}\\t{min(mean0)} \\nPulse Mean\\t: {max(mean1)}\\t{min(mean1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 4**: Standardise the data using sklearn  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data_no_class = data.iloc[:,0:-2]\n",
    "\n",
    "data_scaler = preprocessing.StandardScaler().fit(data_no_class)\n",
    "\n",
    "data_std_temp = data_scaler.transform(data_no_class)\n",
    "type(data_std_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 5**: Merge the standardised data with the associated class\n",
    "\n",
    "_This involves rebuilding the data frame we imported, so I loop through the first row in each column to grab the labels then add them to a list and assign the new data frame I've built with the label list_  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data_classes = data['class']\n",
    "labels = []\n",
    "\n",
    "for label in data_no_class.columns:\n",
    "    labels.append(label)\n",
    "\n",
    "data_std = pd.concat([pd.DataFrame(data_std_temp, columns=labels),data_classes], axis='columns')\n",
    "data_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As can be seen we've recreated the initial data set but it is now standardised  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 6**: Reassess the standard deviations and means\n",
    "\n",
    "_While the means are not all 0, the means are all of order 1 and the deviations are of order 1 as well which means that our model will not be swayed by outliers in our data as much_  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "dev0 = data_std['std_dm']\n",
    "dev1 = data_std['std_pf']\n",
    "mean0 = data_std['mean_dm']\n",
    "mean1 = data_std['mean_pf']\n",
    "\n",
    "print(f'Dispersion Dev  : {max(dev0)}\\t{min(dev0)} \\nPulse Dev\\t: {max(dev1)}\\t{min(dev1)}')\n",
    "print(f'\\n\\nDispersion Mean : {max(mean0)}\\t{min(mean0)} \\nPulse Mean\\t: {max(mean1)}\\t{min(mean1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Q2**\n",
    "\n",
    "**Step 1**: Import the random forest module  \n",
    "\n",
    "**Step 2**: Separate training data from test data; I will employ a 50/50 split  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as classify\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data_std.iloc[:,0:-2], data_std['class'], train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 3**: Train my model with my training data sets  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "model1 = classify().fit(Xtrain, ytrain)\n",
    "prediction1 = model1.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 4**: Test my the efficacy of my model using a confusion matrix as I'm predicting classes  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(ytest, prediction1)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "print(f'model has accuracy of {100*accuracy_score(ytest, prediction1):.1f}% on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Q3**\n",
    "\n",
    "**Step 1**: obtain the list of parameters for my model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV as gridsearch\n",
    "\n",
    "classify().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 2**: Based off Lab 6 I could pick the most important parameters to loop through and optimise my model, or I could just optimise for all hyper parameters. Looking through the list I chose to optimise for `criterion`,`max_depth`,`min_samples_leaf`, and `n_estimators` due to time constraints in optimising the data.\n",
    "\n",
    "**NOTE: THIS BLOCK IS INCREDIBLY SLOW DUE TO THE NUMBER OF PARAMETERS OPTIMISED (40 MINS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    " 'max_depth': np.arange(2, 10),\n",
    " 'min_samples_leaf': np.arange(1, 10),\n",
    " 'n_estimators': np.arange(10,100, 10),\n",
    " }\n",
    "\n",
    "grid = gridsearch(classify(), param_grid)\n",
    "\n",
    "grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 3**: obtain optimised parameters  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 4**: Retest the model with the optimised parameters  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "model2 = classify(criterion='entropy', max_depth=8, min_samples_leaf=1, n_estimators=50).fit(Xtrain,ytrain)\n",
    "prediction2 = model2.predict(Xtest)\n",
    "\n",
    "print(f'model has accuracy of {100*accuracy_score(ytest, prediction2):.1f}% on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step 5**: Reasses the effectiveness of my model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(ytest, prediction2)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_**Observations**: after optimising the data, the model has a false positive percentage of 0.6% while it has a false negative rate of nearly 20%. This implies that the model is good at identifying RFI but not ideal in identifying true pulsars. This is most likely because the subset of data that correlates to true pulsars is a much smaller subset of all of our data._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Q4**\n",
    "\n",
    "**Break down**: This task simply requires me to apply the learning curve function to my model in **Q2**, `model1`. The function returns my training size, the training score, and the validation score for each size. All that is left is graphing my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "test_sizes = np.linspace(0.01,0.5,20)\n",
    "\n",
    "N, train_lc, val_lc = learning_curve(model1, Xtest, ytest,\n",
    "                                           train_sizes=test_sizes)\n",
    "train = np.mean(train_lc, axis=1)\n",
    "test = np.mean(val_lc, axis=1)\n",
    "mean = 0.5*(train[-1]+test[-1])\n",
    "\n",
    "\n",
    "plt.plot(N,train, '-', label='Train')\n",
    "plt.plot(N,test, '-', label='Validation')\n",
    "plt.plot([0,N[-1]],[mean,mean], '--', alpha=0.3, color='black')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Training Sizes')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Observations**: it seems as the model converges in its learning curve at approximately 500 samples which is approximately 5% of the total data. This means that no supplementary data is required to validate this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}