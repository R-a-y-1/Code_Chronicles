{"backend_state":"ready","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-74c3ae3b-96d5-4d62-b899-7d1a7981d613.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1715948535461,"trust":true,"type":"settings"}
{"cell_type":"code","end":1712536640481,"id":"531a79","input":"from sklearn.model_selection import GridSearchCV as gridsearch\n\nclassify().get_params()","kernel":"python3","pos":19,"start":1712536640479,"state":"done","type":"cell"}
{"cell_type":"code","end":1712539021037,"id":"bd9b01","input":"param_grid = {'criterion': ['gini', 'entropy'],\n 'max_depth': np.arange(2, 10),\n 'min_samples_leaf': np.arange(1, 10),\n 'n_estimators': np.arange(10,100, 10),\n }\n\ngrid = gridsearch(classify(), param_grid)\n\ngrid.fit(Xtrain, ytrain)","kernel":"python3","pos":21,"scrolled":false,"start":1712536640484,"state":"done","type":"cell"}
{"cell_type":"code","end":1712539231393,"id":"c278fc","input":"grid.best_params_","kernel":"python3","pos":23,"start":1712539231389,"state":"done","type":"cell"}
{"cell_type":"code","end":1712541872334,"id":"a2a052","input":"model2 = classify(criterion='entropy', max_depth=8, min_samples_leaf=1, n_estimators=50).fit(Xtrain,ytrain)\nprediction2 = model2.predict(Xtest)\n\nprint(f'model has accuracy of {100*accuracy_score(ytest, prediction2):.1f}% on test data')","kernel":"python3","pos":25,"start":1712541871708,"state":"done","type":"cell"}
{"cell_type":"code","end":1712541875608,"id":"989ad1","input":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmat = confusion_matrix(ytest, prediction2)\n\nfig = plt.figure(figsize=(9, 9))\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","kernel":"python3","pos":27,"scrolled":false,"start":1712541875323,"state":"done","type":"cell"}
{"cell_type":"code","end":1712598475260,"id":"1a85d3","input":"dev0 = data['std_dm']\ndev1 = data['std_pf']\nmean0 = data['mean_dm']\nmean1 = data['mean_pf']\n\nprint(f'Dispersion Dev  : {max(dev0)}\\t{min(dev0)} \\nPulse Dev\\t: {max(dev1)}\\t{min(dev1)}')\nprint(f'\\n\\nDispersion Mean : {max(mean0)}\\t{min(mean0)} \\nPulse Mean\\t: {max(mean1)}\\t{min(mean1)}')","kernel":"python3","pos":4,"start":1712598475257,"state":"done","type":"cell"}
{"cell_type":"code","end":1712598482323,"id":"135a09","input":"dev0 = data_std['std_dm']\ndev1 = data_std['std_pf']\nmean0 = data_std['mean_dm']\nmean1 = data_std['mean_pf']\n\nprint(f'Dispersion Dev  : {max(dev0)}\\t{min(dev0)} \\nPulse Dev\\t: {max(dev1)}\\t{min(dev1)}')\nprint(f'\\n\\nDispersion Mean : {max(mean0)}\\t{min(mean0)} \\nPulse Mean\\t: {max(mean1)}\\t{min(mean1)}')","kernel":"python3","pos":11,"start":1712598482320,"state":"done","type":"cell"}
{"cell_type":"code","end":1712603274433,"id":"c51715","input":"from sklearn import preprocessing\n\ndata_no_class = data.iloc[:,0:-2]\n\ndata_scaler = preprocessing.StandardScaler().fit(data_no_class)\n\ndata_std_temp = data_scaler.transform(data_no_class)\ntype(data_std_temp)","kernel":"python3","pos":6,"scrolled":false,"start":1712603270451,"state":"done","type":"cell"}
{"cell_type":"code","end":1712603274484,"id":"72a844","input":"data_classes = data['class']\nlabels = []\n\nfor label in data_no_class.columns:\n    labels.append(label)\n\ndata_std = pd.concat([pd.DataFrame(data_std_temp, columns=labels),data_classes], axis='columns')\ndata_std.head()","kernel":"python3","pos":8,"scrolled":false,"start":1712603274444,"state":"done","type":"cell"}
{"cell_type":"code","end":1712603282443,"id":"ab4cee","input":"model1 = classify().fit(Xtrain, ytrain)\nprediction1 = model1.predict(Xtest)\n","kernel":"python3","pos":15,"start":1712603280915,"state":"done","type":"cell"}
{"cell_type":"code","end":1712603285861,"id":"4b04aa","input":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmat = confusion_matrix(ytest, prediction1)\n\nfig = plt.figure(figsize=(9, 9))\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues')\nplt.xlabel('true label')\nplt.ylabel('predicted label');\nprint(f'model has accuracy of {100*accuracy_score(ytest, prediction1):.1f}% on test data')","kernel":"python3","pos":17,"scrolled":false,"start":1712603284217,"state":"done","type":"cell"}
{"cell_type":"code","end":1712608390991,"id":"7f348f","input":"from sklearn.ensemble import RandomForestClassifier as classify\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nXtrain, Xtest, ytrain, ytest = train_test_split(data_std.iloc[:,0:-2], data_std['class'], train_size=0.5)","kernel":"python3","pos":13,"scrolled":false,"start":1712608390982,"state":"done","type":"cell"}
{"cell_type":"code","end":1712610688911,"id":"c428ae","input":"from sklearn.model_selection import learning_curve\n\ntest_sizes = np.linspace(0.01,0.5,20)\n\nN, train_lc, val_lc = learning_curve(model1, Xtest, ytest,\n                                           train_sizes=test_sizes)\ntrain = np.mean(train_lc, axis=1)\ntest = np.mean(val_lc, axis=1)\nmean = 0.5*(train[-1]+test[-1])\n\n\nplt.plot(N,train, '-', label='Train')\nplt.plot(N,test, '-', label='Validation')\nplt.plot([0,N[-1]],[mean,mean], '--', alpha=0.3, color='black')\nplt.ylabel('Score')\nplt.xlabel('Training Sizes')\nplt.legend()","kernel":"python3","pos":30,"scrolled":false,"start":1712610656400,"state":"done","type":"cell"}
{"cell_type":"code","end":1713192189857,"id":"565dd7","input":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('./data/pulsar.csv', index_col=0)\nnumber_RFI = len(data[data['class'] == 0])\nnumber_pulsars = len(data[data['class'] == 1])\n\nprint(f'The number of pulsars is {number_pulsars}, and the number of noise data points is {number_RFI}')","kernel":"python3","pos":2,"scrolled":false,"start":1713192189832,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"05f034","input":"_**Observations**: after optimising the data, the model has a false positive percentage of 0.6% while it has a false negative rate of nearly 20%. This implies that the model is good at identifying RFI but not ideal in identifying true pulsars. This is most likely because the subset of data that correlates to true pulsars is a much smaller subset of all of our data._\n","pos":28,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0d4b2d","input":"**Step 5**: Reasses the effectiveness of my model  \n\n","pos":26,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"154478","input":"**Observations**: it seems as the model converges in its learning curve at approximately 500 samples which is approximately 5% of the total data. This means that no supplementary data is required to validate this model.","pos":31,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"19c931","input":"**Step 6**: Reassess the standard deviations and means\n\n_While the means are not all 0, the means are all of order 1 and the deviations are of order 1 as well which means that our model will not be swayed by outliers in our data as much_  \n\n","pos":10,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"20a047","input":"As can be seen we've recreated the initial data set but it is now standardised  \n\n","pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"28bea5","input":"**Step 3**: obtain optimised parameters  \n\n","pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"308c72","input":"**Step 2**: Based off Lab 6 I could pick the most important parameters to loop through and optimise my model, or I could just optimise for all hyper parameters. Looking through the list I chose to optimise for `criterion`,`max_depth`,`min_samples_leaf`, and `n_estimators` due to time constraints in optimising the data.\n\n**NOTE: THIS BLOCK IS INCREDIBLY SLOW DUE TO THE NUMBER OF PARAMETERS OPTIMISED (40 MINS)**","pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3a54ac","input":"**Step 4**: Standardise the data using sklearn  \n\n","pos":5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"672a7d","input":"# **Q1**\n\n**Step 1**: Load in the the csv into a `dataframe`.  \n\n**Step 2**: Assess how many unique values of class there are.  \n\n_Note: cannot query data using_ _`data.query()`_ _as 'class' is a python keyword_  \n\n","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"9d831a","input":"**Step 5**: Merge the standardised data with the associated class\n\n_This involves rebuilding the data frame we imported, so I loop through the first row in each column to grab the labels then add them to a list and assign the new data frame I've built with the label list_  \n\n","pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a67d97","input":"# **Q4**\n\n**Break down**: This task simply requires me to apply the learning curve function to my model in **Q2**, `model1`. The function returns my training size, the training score, and the validation score for each size. All that is left is graphing my data.","pos":29,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bcd853","input":"# **Q3**\n\n**Step 1**: obtain the list of parameters for my model  \n\n","pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c0869b","input":"**Step 4**: Test my the efficacy of my model using a confusion matrix as I'm predicting classes  \n\n","pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"cb0bf6","input":"**Step 4**: Retest the model with the optimised parameters  \n\n","pos":24,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e31b7c","input":"# **Q2**\n\n**Step 1**: Import the random forest module  \n\n**Step 2**: Separate training data from test data; I will employ a 50/50 split  \n\n","pos":12,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f9e999","input":"**Step 3**: Train my model with my training data sets  \n\n","pos":14,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fc0cfb","input":"**Step 3**: Check the range of standard deviations and means in the data set.\n\n_It seems that the standard deviations are not of order 1, nor are the means of order 0, as such we will have to standardise our data before analysing to make sure the model is not over emphasising the high variance/high mean data._\n\n","pos":3,"state":"done","type":"cell"}
{"id":0,"time":1715947070828,"type":"user"}
{"last_load":1711406999461,"type":"file"}